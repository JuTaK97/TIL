{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN.ipynb","provenance":[],"authorship_tag":"ABX9TyPtknsWD3YsG+D94P4G685Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9P-iSIvHJEev"},"source":["# Recurrent Neural Network\n","기존 neural network에서는 뉴런에 입력 벡터가 들어가면, parameter들과의 계산 후(예: Wx+b) activation function을 지나 출력되었다. <br />\n","RNN에서는 새로운 가중치로 '과거의 자신'이 추가된다. 따라서 은닉층의 메모리 셀은 tanh(Wx+W'h+b)와 같이 계산된다. <br />\n","이때 Wx는 기존 뉴런처럼 입력 벡터와 그를 위한 가중치를 나타내고, W'h는 이전 메모리 셀의 값 h와 그를 위한 가중치 W'를 나타낸다.<br />\n","h' = tanh(Wx+W'h+b) 로 계산된 값은 출력층에서 또 f(W''h'+b)같이 계산되어 출력된다.<br />\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pgQ3zYw1u2ow"},"source":["## 1. 은닉 셀 구현하기"]},{"cell_type":"code","metadata":{"id":"_Rl4YiDMJCJO"},"source":["import numpy as np\n","\n","timesteps = 10      # 시점의 수. input vector들이 입력되는 횟수이다.\n","input_size = 4      # input vector의 size\n","hidden_size = 8     # 은닉층의 메모리 셀의 용량\n","\n","inputs = np.random.random( (timesteps, input_size) )   # 랜덤으로, 입력될 값 10개 (각각 크기 4) 생성 \n","hidden_state_t = np.zeros((hidden_size,))              # 각 은닉층의 메모리 셀은 0으로 초기화\n","\n","Wx = np.random.random( (hidden_size, input_size) )     # 입력 벡터에 대한 가중치 (8,4)\n","Wh = np.random.random( (hidden_size, hidden_size) )     # 은닉 상태(이전 t의 자신)에 대한 가중치 (8,8)\n","b = np.random.random( (hidden_size,)) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UXvshzKMWlrc","executionInfo":{"status":"ok","timestamp":1633744072055,"user_tz":-540,"elapsed":278,"user":{"displayName":"주현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5ltk6mU4Acire5podT_1jZ1HY_FA1wdtKouHBCA=s64","userId":"04151192621211208729"}},"outputId":"34b5e5e5-3392-4e46-9f86-c6c308f914e4"},"source":["total_hidden_states = []\n","\n","for input_t in inputs :\n","  output_t = np.tanh(np.dot(Wx, input_t)+np.dot(Wh, hidden_state_t)+b)\n","  total_hidden_states.append(list(output_t))\n","  hidden_state_t = output_t\n","\n","print(np.stack(total_hidden_states, axis=0))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.95303276 0.90343227 0.88426935 0.94407579 0.95485008 0.9252148\n","  0.87426494 0.9785663 ]\n"," [0.99987321 0.99983392 0.99999654 0.99999751 0.99999214 0.99991579\n","  0.99995054 0.99999042]\n"," [0.99993685 0.99956887 0.99999522 0.99999701 0.99999001 0.99993005\n","  0.99994002 0.99999203]\n"," [0.9999174  0.9999021  0.99999894 0.99999863 0.99999676 0.99997041\n","  0.99998745 0.99999731]\n"," [0.99996926 0.99997711 0.99999967 0.99999958 0.99999904 0.99998845\n","  0.99999531 0.9999992 ]\n"," [0.9998565  0.99972533 0.99999709 0.99999716 0.99999242 0.99997541\n","  0.99998534 0.99999837]\n"," [0.99991267 0.99978131 0.99999755 0.99999786 0.99999391 0.99995364\n","  0.99997264 0.99999558]\n"," [0.99991645 0.99993062 0.99999932 0.9999988  0.99999754 0.99998092\n","  0.99999313 0.99999838]\n"," [0.99994763 0.99993819 0.99999937 0.999999   0.99999785 0.99998237\n","  0.99999264 0.99999844]\n"," [0.99996132 0.99992886 0.99999849 0.99999935 0.99999758 0.99996982\n","  0.99997817 0.99999818]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"P7LMIASQu9sg"},"source":["##2. nn.RNN()으로 구현하기"]},{"cell_type":"code","metadata":{"id":"wO54dT77u634","executionInfo":{"status":"ok","timestamp":1633767707872,"user_tz":-540,"elapsed":26533,"user":{"displayName":"주현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5ltk6mU4Acire5podT_1jZ1HY_FA1wdtKouHBCA=s64","userId":"04151192621211208729"}}},"source":["import torch\n","import torch.nn as nn"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"54JEL_nhx_5n"},"source":["#### 은닉층 1개"]},{"cell_type":"code","metadata":{"id":"ozP-oqjpvBqv","executionInfo":{"status":"ok","timestamp":1633750362983,"user_tz":-540,"elapsed":2,"user":{"displayName":"주현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5ltk6mU4Acire5podT_1jZ1HY_FA1wdtKouHBCA=s64","userId":"04151192621211208729"}}},"source":["input_size = 5\n","hidden_size = 8\n","\n","inputs = torch.Tensor(1, 10, 5)     # 배치 1개, 시점 10개, 각각 크기 5\n","cell = nn.RNN(input_size, hidden_size, batch_first=True)    # batch_first=True는 input의 첫 성분이 배치 크기라는 뜻\n","outputs, _status = cell(inputs)     # output: 모든 시점의 은닉값들, _status: 최종 시점의 은닉값\n","                                    # output: (1, 10, 8)            _status: (1, 1, 8)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N4FFdHJxyCiX"},"source":["#### 은닉층 여러개"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MCyamBwbx6Zu","executionInfo":{"status":"ok","timestamp":1633751595982,"user_tz":-540,"elapsed":312,"user":{"displayName":"주현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5ltk6mU4Acire5podT_1jZ1HY_FA1wdtKouHBCA=s64","userId":"04151192621211208729"}},"outputId":"510f3d4d-cf1e-4995-df79-cbef216aadb9"},"source":["inputs = torch.Tensor(1, 10, 5)\n","cell = nn.RNN(input_size=5, hidden_size=8, num_layers=4, batch_first=True)     # 4개의 은닉층\n","outputs, _status = cell(inputs)\n","\n","print(outputs.shape)    # 모든 시점의 hidden state들. 근데 마지막 층의 hidden state만 출력된다. 따라서 (1, 10, 8)로 동일\n","print(_status.shape)    # 마지막 시점의 hidden state. layer가 4개이니 (4, 1, 8)이 된다."],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 10, 8])\n","torch.Size([4, 1, 8])\n"]}]},{"cell_type":"markdown","metadata":{"id":"g-fYGXfWL7wq"},"source":["##3. Bidirectional Recurrent Neural Network\n","한 timestep마다 메모리 셀을 두 개 사용한다. <br />\n","하나는 RNN과 동일하게 이전 시점의 hidden state를 받아오고, 나머지 하나는 이후 시점의 hidden state를 받아온다.<br />\n","그리고 출력은 이 두 셀을 모두 사용해서 이루어진다.<br />\n","layer가 늘어난다면, 아래 layer의 두 셀과 위 layer의 두 셀이 모두 얽혀서 전달되게 된다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wb1O-oCKL_go","executionInfo":{"status":"ok","timestamp":1633753249306,"user_tz":-540,"elapsed":298,"user":{"displayName":"주현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5ltk6mU4Acire5podT_1jZ1HY_FA1wdtKouHBCA=s64","userId":"04151192621211208729"}},"outputId":"3195520d-434f-4963-d07b-2a94c4a9d03c"},"source":["inputs = torch.Tensor(1, 10, 5)     # 배치 1개, 시점 10개, 각각 size 5\n","cell = nn.RNN(input_size=5, hidden_size=8, num_layers=3, batch_first=True, bidirectional=True)    # bidirectional=True를 추가하면 양방향 RNN이 된다.\n","outputs, _status = cell(inputs)\n","\n","print(outputs.shape)    # 모든 시점, 마지막 layer의 hidden state이다. 따라서 (1, 10, 16)이다.\n","                        # 10은 시점 개수, 16은 hidden_size의 두 배이다. 메모리 셀이 두 개씩 있고 그 두개를 concatenate한 게 출력된다\n","\n","print(_status.shape)    # 마지막 시점, 모든 layer의 hidden state이다. 따라서 (6, 1, 8))이다.\n","                        # 6은 layer 개수 3의 두배이다. (메모리 셀이 2개)  timestep은 하나니까 1, 그리고 각 hidden state의 size는 8."],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 10, 16])\n","torch.Size([6, 1, 8])\n"]}]},{"cell_type":"markdown","metadata":{"id":"g8kPg3V9SVVB"},"source":["# LSTM(Long Short-Term Memory)\n","<br />\n","RNN의 한계는 timestep이 진행될 수록 이전 input들이 잊혀진다는 것이다. 즉 RNN은 기억력이 부족하다고 할 수 있다.<br />\n","이를 장기 의존성 문제 라고 한다. 그리고 이것을 해결할 방법이 LSTM이다.<br />\n","\n","## 내부 구조\n","1. 입력 게이트<br />\n","  현재 정보를 기억하기 위한 게이트이다.<br />\n","  1-1. 현재 시점의 input과 이전 시점의 hidden state에 각각 가중치를 곱해서 __$tanh$__ 의 activation function에 통과시킨 것이 $g_t$가 된다.<br />\n","  1-2. 현재 시점의 input과 이전 시점의 hidden state에 각각 가중치를 곱해서 __$\\sigma$__의 activation function에 통과시킨 것이 $i_t$가 된다.<br /><br />\n","2. 삭제 게이트<br />\n","  현재 시점의 input과 이전 시점의 hidden state에 각각 가중치를 곱해서 __$\\sigma$__를 통과하는데, 결과로 0~1의 값이 나온다.<br />\n","  이 결과를 $f_t$라고 하고 이 값이 0에 가까울 수록 '기억'을 많이 지우게 된다.<br /><br />\n","3. 셀 상태(장기 상태)<br />\n","  3-1. 이전 셀 상태 $C_{t-1}$과 $f_t$의 entrywise 곱을 한다.<br />\n","  3-2. $g_t$와 $i_t$도 entrywise 곱을 한다.<br />\n","  3-3. 이 두 값을 더한 것이 t시점의 셀 상태 $C_t$가 된다.<br />\n","  삭제 게이트의 값에 따라 과거의 값에 얼마나 의존할지 결정된다.<br /><br />\n","4. 출력 게이트<br />\n","  4-1. 현재 시점의 input과 이전 시점의 hidden state에 각각 가중치를 곱해서 __$\\sigma$__를 통과시킨다. 이를 $o_t$라고 한다.<br />\n","  4-2. $o_t$와 $tanh(c_t)$의 entrywise 곱이 현재 시점의 은닉값(단기 상태)가 된다. 즉 t시점의 hidden state $h_t$가 되고, 이는 다음 timestep으로<br />\n","전달되고, 다음 layer(혹은 출력층)로도 전달되게 된다.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XuD4rpaCJTaT"},"source":["Pytorch에서는 간단하게 구현할 수 있다."]},{"cell_type":"code","metadata":{"id":"0DB7Df2WSYxR","executionInfo":{"status":"ok","timestamp":1633767728182,"user_tz":-540,"elapsed":309,"user":{"displayName":"주현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5ltk6mU4Acire5podT_1jZ1HY_FA1wdtKouHBCA=s64","userId":"04151192621211208729"}}},"source":["input_dim=5\n","hidden_size=8\n","\n","cell = nn.LSTM(input_dim, hidden_size, batch_first=True)  "],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6TJoB8faNz8C"},"source":["# RNN으로 간단한 모델 구현하기\n","문자 단위 RNN"]},{"cell_type":"code","metadata":{"id":"TahrwIpJN0Nx","executionInfo":{"status":"ok","timestamp":1633774718477,"user_tz":-540,"elapsed":254,"user":{"displayName":"주현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5ltk6mU4Acire5podT_1jZ1HY_FA1wdtKouHBCA=s64","userId":"04151192621211208729"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np"],"execution_count":91,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dHr_zcxmODLB","executionInfo":{"status":"ok","timestamp":1633774719709,"user_tz":-540,"elapsed":269,"user":{"displayName":"주현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5ltk6mU4Acire5podT_1jZ1HY_FA1wdtKouHBCA=s64","userId":"04151192621211208729"}},"outputId":"8fd5111f-703f-44af-b851-e689ed67e998"},"source":["input_str = 'recurrent'\n","label_str = 'neuralnet'\n","char_vocab = sorted(list(set(input_str+label_str)))\n","vocab_size = len(char_vocab)\n","print(vocab_size)"],"execution_count":92,"outputs":[{"output_type":"stream","name":"stdout","text":["8\n"]}]},{"cell_type":"code","metadata":{"id":"-0hpgLwUO_1Y","executionInfo":{"status":"ok","timestamp":1633774721062,"user_tz":-540,"elapsed":305,"user":{"displayName":"주현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5ltk6mU4Acire5podT_1jZ1HY_FA1wdtKouHBCA=s64","userId":"04151192621211208729"}}},"source":["input_size = vocab_size  \n","hidden_size = 8\n","output_size = vocab_size\n","learning_rate=0.1"],"execution_count":93,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mnMctk2FPIIR","executionInfo":{"status":"ok","timestamp":1633774723259,"user_tz":-540,"elapsed":284,"user":{"displayName":"주현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5ltk6mU4Acire5podT_1jZ1HY_FA1wdtKouHBCA=s64","userId":"04151192621211208729"}},"outputId":"d694aece-1c4e-4c31-b294-033b0dce2c70"},"source":["char_to_index = dict((c,i) for i, c in enumerate(char_vocab))\n","\n","index_to_char = {}\n","for key, value in char_to_index.items() :\n","  index_to_char[value] = key\n","\n","\n","x_data = [char_to_index[c] for c in input_str]\n","y_data = [char_to_index[c] for c in label_str]\n","\n","x_data = [x_data]   # 배치 차원 추가\n","y_data = [y_data]   # 배치 차원 추가\n","\n","x_one_hot = [np.eye(vocab_size)[x] for x in x_data] # one-hot encoding\n","\n","X = torch.FloatTensor(x_one_hot)\n","Y = torch.LongTensor(y_data)\n","\n","print('훈련 데이터의 크기 : {}'.format(X.shape))    # 9개 문자가, 12차원 one-hot encode되었기 때문에 (1,9,12)\n","print('레이블의 크기 : {}'.format(Y.shape))"],"execution_count":94,"outputs":[{"output_type":"stream","name":"stdout","text":["훈련 데이터의 크기 : torch.Size([1, 9, 8])\n","레이블의 크기 : torch.Size([1, 9])\n"]}]},{"cell_type":"code","metadata":{"id":"2UzuqxbETKkY","executionInfo":{"status":"ok","timestamp":1633774725174,"user_tz":-540,"elapsed":284,"user":{"displayName":"주현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5ltk6mU4Acire5podT_1jZ1HY_FA1wdtKouHBCA=s64","userId":"04151192621211208729"}}},"source":["class Net(torch.nn.Module) :\n","  def __init__(self, input_size, hidden_size, output_size) :\n","    super(Net, self).__init__()\n","    self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True)\n","    self.fc = torch.nn.Linear(hidden_size, output_size, bias=True)  # 출력층으로 전결합층 사용\n","\n","  def forward(self, x) :\n","    x, _status = self.rnn(x)\n","    x = self.fc(x)\n","    return x"],"execution_count":95,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gpke4B8pUDQ3","executionInfo":{"status":"ok","timestamp":1633774765475,"user_tz":-540,"elapsed":403,"user":{"displayName":"주현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5ltk6mU4Acire5podT_1jZ1HY_FA1wdtKouHBCA=s64","userId":"04151192621211208729"}},"outputId":"176110c8-a791-4b13-d1a9-5d98c36d6c22"},"source":["net = Net(input_size, hidden_size, output_size)\n","\n","outputs = net(X)\n","print(outputs.shape)   # (1, 9, 9)   배치 1개, 시점은 9개(input string이 9문자니까), output size 9.\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(net.parameters(), learning_rate)\n","\n","for epoch in range(101) :\n","  optimizer.zero_grad()\n","  output = net(X)\n","  loss = criterion(output.view(-1, input_size), Y.view(-1))     # batch 차원을 없애고 input_size 차원과 합친다.\n","  loss.backward()\n","  optimizer.step()\n","\n","  if epoch%20==0 :\n","    result = output.data.numpy().argmax(axis=2)\n","    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n","    print(epoch, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"],"execution_count":98,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 9, 8])\n","0 loss:  2.090449333190918 prediction:  [[5 2 2 2 2 5 2 2 5]] true Y:  [[4, 2, 7, 5, 0, 3, 4, 2, 6]] prediction str:  reeeereer\n","20 loss:  0.013998348265886307 prediction:  [[4 2 7 5 0 3 4 2 6]] true Y:  [[4, 2, 7, 5, 0, 3, 4, 2, 6]] prediction str:  neuralnet\n","40 loss:  0.0016270828200504184 prediction:  [[4 2 7 5 0 3 4 2 6]] true Y:  [[4, 2, 7, 5, 0, 3, 4, 2, 6]] prediction str:  neuralnet\n","60 loss:  0.0009611963760107756 prediction:  [[4 2 7 5 0 3 4 2 6]] true Y:  [[4, 2, 7, 5, 0, 3, 4, 2, 6]] prediction str:  neuralnet\n","80 loss:  0.0007754646358080208 prediction:  [[4 2 7 5 0 3 4 2 6]] true Y:  [[4, 2, 7, 5, 0, 3, 4, 2, 6]] prediction str:  neuralnet\n","100 loss:  0.0006671748124063015 prediction:  [[4 2 7 5 0 3 4 2 6]] true Y:  [[4, 2, 7, 5, 0, 3, 4, 2, 6]] prediction str:  neuralnet\n"]}]}]}