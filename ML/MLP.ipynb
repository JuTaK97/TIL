{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLP.ipynb","provenance":[],"authorship_tag":"ABX9TyN+rVN9bUrn2JXHv2n5HuUY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"J03u-oGeG7x8"},"source":["# 1. MLP로 XOR 구현하기"]},{"cell_type":"code","metadata":{"id":"NQxTPHoEG5qW","executionInfo":{"status":"ok","timestamp":1633683408107,"user_tz":-540,"elapsed":253,"user":{"displayName":"주현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5ltk6mU4Acire5podT_1jZ1HY_FA1wdtKouHBCA=s64","userId":"04151192621211208729"}}},"source":["import torch\n","import torch.nn as nn\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","torch.manual_seed(777)\n","if device == 'cuda':\n","    torch.cuda.manual_seed_all(777)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AqqUv8MNKjZS"},"source":["#### 1.1 단층 퍼셉트론 시도"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2H3s5nV_HCX8","executionInfo":{"status":"ok","timestamp":1633683587345,"user_tz":-540,"elapsed":8093,"user":{"displayName":"주현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5ltk6mU4Acire5podT_1jZ1HY_FA1wdtKouHBCA=s64","userId":"04151192621211208729"}},"outputId":"bb10ef93-37bb-41bb-cec1-0f8255591528"},"source":["X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n","Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n","\n","linear = nn.Linear(2,1)\n","sigmoid = nn.Sigmoid()\n","model = nn.Sequential(linear, sigmoid).to(device)\n","\n","criterion = torch.nn.BCELoss().to(device)               # 이진 분류에 사용하는 Cross-Entropy 함수\n","optimizer = torch.optim.SGD(model.parameters(), lr=1)\n","\n","for step in range(10001) :\n","\n","  optimizer.zero_grad()\n","  hyp = model(X)\n","  cost = criterion(hyp, Y)\n","  cost.backward()\n","  optimizer.step()\n","\n","  if step % 2000 == 0: \n","      print(step, cost.item())  # 줄어들지 않는다. XOR은 단층 퍼셉트론으로는 풀 수 없다."],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["0 0.70228111743927\n","2000 0.6931471824645996\n","4000 0.6931471824645996\n","6000 0.6931471824645996\n","8000 0.6931471824645996\n","10000 0.6931471824645996\n"]}]},{"cell_type":"markdown","metadata":{"id":"TP31_U9FKmVh"},"source":["####1.2 다층 퍼셉트론 설계"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kh9uaIIrKogS","executionInfo":{"status":"ok","timestamp":1633684703024,"user_tz":-540,"elapsed":25660,"user":{"displayName":"주현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5ltk6mU4Acire5podT_1jZ1HY_FA1wdtKouHBCA=s64","userId":"04151192621211208729"}},"outputId":"3d431e72-44db-4bb3-ec17-370c70f5c58c"},"source":["X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n","Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n","\n","model = nn.Sequential(\n","      nn.Linear(2, 10, bias=True),    # 입력\n","      nn.Sigmoid(),\n","      nn.Linear(10,10, bias=True),    # hidden 1\n","      nn.Sigmoid(),\n","      nn.Linear(10,10, bias=True),    # hidden 2\n","      nn.Sigmoid(),\n","      nn.Linear(10, 1, bias=True),    # 출력\n","      nn.Sigmoid()                    \n",").to(device)\n","\n","criterion = torch.nn.BCELoss().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n","\n","for epoch in range(10001) :\n","\n","  hyp = model(X)\n","  cost = criterion(hyp, Y)\n","\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","\n","  if epoch % 2000 == 0:\n","      print(epoch, cost.item())"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["0 0.7006749510765076\n","2000 5.9339705330785364e-05\n","4000 1.3590030903287698e-05\n","6000 4.321354026615154e-06\n","8000 1.4901181657478446e-06\n","10000 5.662444095833052e-07\n"]}]},{"cell_type":"markdown","metadata":{"id":"MHB9UjMwMv1B"},"source":["#### 1.3 학습한 MLP 성능 Test"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ez-vD7h_Mvf6","executionInfo":{"status":"ok","timestamp":1633684873930,"user_tz":-540,"elapsed":274,"user":{"displayName":"주현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5ltk6mU4Acire5podT_1jZ1HY_FA1wdtKouHBCA=s64","userId":"04151192621211208729"}},"outputId":"a902776c-2bfc-45f7-eb8b-c19b2a525303"},"source":["X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n","Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n","\n","with torch.no_grad() :\n","  hyp = model(X)\n","  pred = (hyp>0.5).float()\n","  accuracy = (pred==Y).float().mean()\n","\n","  print('모델의 출력값(Hypothesis): ', hyp.detach().cpu().numpy())\n","  print('모델의 예측값(Predicted): ', pred.detach().cpu().numpy())\n","  print('실제값(Y): ', Y.cpu().numpy())\n","  print('정확도(Accuracy): ', accuracy.item())"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["모델의 출력값(Hypothesis):  [[3.8152713e-08]\n"," [9.9999893e-01]\n"," [9.9999893e-01]\n"," [3.8867977e-08]]\n","모델의 예측값(Predicted):  [[0.]\n"," [1.]\n"," [1.]\n"," [0.]]\n","실제값(Y):  [[0.]\n"," [1.]\n"," [1.]\n"," [0.]]\n","정확도(Accuracy):  1.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"42XRRgpnPl3B"},"source":["## 2. MLP로 MNIST 분류하기"]},{"cell_type":"markdown","metadata":{"id":"JBSXToFzQGW3"},"source":["#### 2.1 데이터 가져오기"]},{"cell_type":"code","metadata":{"id":"b0aLhf2hPqao","executionInfo":{"status":"ok","timestamp":1633685565393,"user_tz":-540,"elapsed":30560,"user":{"displayName":"주현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh5ltk6mU4Acire5podT_1jZ1HY_FA1wdtKouHBCA=s64","userId":"04151192621211208729"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","% matplotlib inline\n","from sklearn.datasets import fetch_openml\n","\n","mnist = fetch_openml('mnist_784', version=1, cache=True)\n","mnist.target = mnist.target.astype(np.int8)\n","\n","X = mnist.data / 256\n","Y = mnist.target\n","\n"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oa0rLAwETLvQ"},"source":["# 3. Overfitting 방지하기\n","\n","## 3.1 모델 복잡도 줄이기\n","## 3.2 Regularization 추가하기\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-6, weight_decay=1e-5)\n","## 3.3 Dropout\n","Training 과정에서 일부러 몇 개 뉴런을 사용하지 않는 것. <br />\n","효과 1. 특정 조합에 너무 의존적이게 되는 것을 방지<br />\n","효과 2. 랜덤하게 dropout하면, 서로 다른 여러 신경망의 앙상블을 사용하는 것과 같은 효과가 있다\n","\n"]}]}